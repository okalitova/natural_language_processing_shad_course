{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание на семинар 28 сентября по курсу анализа текстов ШАД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тема: Определение частей речи с помощью скрытой марковской модели и алгоритма Витерби. \n",
    "\n",
    "\n",
    "\n",
    "**Дедлайн**:   9:00 утра 5 октября (для заочников 9.00 утра 8 октября)\n",
    "\n",
    "**Среда выполнения**: Jupyter Notebook (Python 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "В данной  работе вам предстоит:\n",
    "\n",
    "- обучить скрытую марковскую модель на размеченных данных\n",
    "- реализовать алгоритм Витерби для декодирования\n",
    "\n",
    "Задание сдается в контест."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение частей речи (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем решать задачу определения частей речи (POS-теггинга) с помощью скрытой марковской модели (HMM). Формула совместной плотности наблюдаемых и скрытых переменных задается как\n",
    "\n",
    "$$ p(X, T) = p(T) p(X|T) = p(t_1)  \\prod_{i=2}^N p(t_i|t_{i-1}) \\prod_{i=1}^N p(x_i|t_i)$$\n",
    "\n",
    "В данном случае:\n",
    "\n",
    "- наблюдаемые переменные $X$ - это слова корпуса;\n",
    "\n",
    "- скрытые переменные $T$ - это POS-теги."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Обучение HMM на размеченных данных\n",
    "\n",
    "Требуется построить скрытую марковскую модель и настроить все ее параметры с помощью оценок максимального правдоподобия по размеченным данным (последовательности пар слово+тег):\n",
    "\n",
    "- Вероятности переходов между скрытыми состояниями $p(t_i | t_{i - 1})$ посчитайте на основе частот биграмм POS-тегов.\n",
    "\n",
    "- Вероятности эмиссий наблюдаемых состояний $p(x_i | t_i)$ посчитайте на основе частот \"POS-тег - слово\".\n",
    "\n",
    "- Обратите внимание на проблему разреженности счетчиков и сделаейте все вероятности сглаженными по Лапласу (add-one smoothing).\n",
    "\n",
    "- Распределение вероятностей начальных состояний $p(t_1)$ задайте равномерным.\n",
    "\n",
    "Обратите внимание, что так как мы используем размеченные данные, то у нас нет необходимости в оценивании апостериорных вероятностей на скрытые переменные с помощью алгоритма forward-backword и использовании EM-алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите brown корпус с универсальной системой тегирования. Для этого вам понадобятся ресурсы brown и universal_tagset из nltk.download().  В этой системе содержатся следующие теги:\n",
    "\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition\t(on, of, at, ...)\n",
    "- ADV - adverb\t(really, already, still, ...)\n",
    "- CONJ\t- conjunction\t(and, or, but, ...)\n",
    "- DET - determiner, article\t(the, a, some, ...)\n",
    "- NOUN\t- noun\t(year, home, costs, ...)\n",
    "- NUM - numeral\t(twenty-four, fourth, 1991, ...)\n",
    "- PRT -\tparticle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- .\t- punctuation marks\t(. , ;)\n",
    "- X\t- other\t(ersatz, esprit, dunno, ...)\n",
    "\n",
    "Тегсеты в корпусах текстов и в различных теггерах могут быть разными. Но есть маппер: http://www.nltk.org/_modules/nltk/tag/mapping.html\n",
    "\n",
    "Проанализируйте данные, с которыми Вы работаете. В частности, ответьте на вопросы:\n",
    "- Каков общий объем датасета, формат?\n",
    "- Приведены ли слова к нижнему регистру? Чем  это нам может в дальнейшем помешать?\n",
    "- Как распределены слова в корпусе?  Как распределены теги в корпусе?\n",
    "\n",
    "Сделайте случайное разбиение выборки на обучение и контроль в отношении 9:1 и обучите скрытую марковскую модель из предыдущего пункта. Если впоследствии обучение моделей будет занимать слишком много времени, работайте с подвыборкой, например, только текстами определенных категорий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(tagset=\"universal\")\n",
    "\n",
    "\n",
    "def train_hmm(tagged_sents):\n",
    "    \"\"\"\n",
    "    Calucaltes p(tag), p(word|tag), p(tag|tag) from corpus.\n",
    "\n",
    "    Args:\n",
    "        tagged_sents: list of list of tagged tokens. \n",
    "            Example: \n",
    "            [[('dog', 'NOUN'), ('eats', 'VERB'), ...], ...]\n",
    "\n",
    "    Returns:\n",
    "        p_t, p_w_t, p_t_t - tuple of 3 elements:\n",
    "        p_t - dict(float), tag -> proba\n",
    "        p_w_t - dict(dict(float), tag -> word -> proba\n",
    "        p_t_t - dict(dict(float), previous_tag -> tag -> proba\n",
    "    \"\"\"\n",
    "    alpha=1e-24\n",
    "    counter_tag = Counter()\n",
    "    counter_tag_tag = Counter()\n",
    "    counter_tag_word = Counter()\n",
    "    tags = set()\n",
    "    words = set()\n",
    "    p_t_t = defaultdict(dict)\n",
    "    p_w_t = defaultdict(dict)\n",
    "    p_t = dict()\n",
    "    \n",
    "    for tagged_sent in tagged_sents:\n",
    "        for i, word_tag in enumerate(tagged_sent):\n",
    "            word, tag = word_tag\n",
    "            tags.add(tag)\n",
    "            words.add(word)\n",
    "            counter_tag_word[(tag, word)] += 1\n",
    "            counter_tag[tag] += 1\n",
    "            if i > 0:\n",
    "                counter_tag_tag[(tagged_sent[i - 1][1], tag)] += 1\n",
    "    \n",
    "    for prev_tag_tag in counter_tag_tag.keys():\n",
    "        prev_tag, tag = prev_tag_tag\n",
    "        p_t_t[prev_tag][tag] = (counter_tag_tag[(prev_tag, tag)] + alpha)\\\n",
    "        / (counter_tag[prev_tag] + alpha * len(tags))\n",
    "    \n",
    "    for tag_word in counter_tag_word.keys():\n",
    "        tag, word = tag_word\n",
    "        p_w_t[tag][word] = (counter_tag_word[(tag, word)] + alpha) \\\n",
    "        / (counter_tag[tag] + alpha * len(words))\n",
    "        \n",
    "    for tag in counter_tag.keys():\n",
    "        p_t[tag] = 1 / len(tags)\n",
    "    \n",
    "    return p_t, p_w_t, p_t_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Алгоритм Витерби для применения модели\n",
    "\n",
    "Чтобы использовать обученную модель для определения частей речи на новых данных, необходимо реализовать алгоритм Витерби. Это алгоритм динамиеского программирования, с помощью которого мы будем находить наиболее вероятную последовательность скрытых состояний модели для фиксированной последовательности слов:\n",
    "\n",
    "$$ \\hat{T} = \\arg \\max_{T} p(T|X) = \\arg \\max_{T} p(X, T) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, определяющую максимальную вероятность последовательности, заканчивающейся на $i$-ой позиции в состоянии $k$:\n",
    "\n",
    "$$\\delta(k, i) = \\max_{t_1, \\dots t_{i-1}} p(x_1, \\dots x_i, t_1, \\dots t_i=k)$$\n",
    "\n",
    "Тогда $\\max_{k} \\delta(k, N)$ - максимальная вероятность всей последовательности. А состояния, на которых эта вероятность достигается - ответ задачи.\n",
    "\n",
    "Алгоритм Витерби заключается в последовательном пересчете функции $\\delta(k, i)$ по формуле:\n",
    "\n",
    "$$\\delta(k, i) = \\max_{m} \\delta(m, i-1) p(t_i = k|t_{i-1} = m) p(x_i|t_i=k) $$\n",
    "\n",
    "Аналогично пересчитывается функция, определяющая, на каком состоянии этот максимум достигается:\n",
    "\n",
    "$$s(k, i) = \\arg \\max_{m} \\delta(m, i-1) p(t_i = k|t_{i-1} = m) p(x_i|t_i=k) $$\n",
    "\n",
    "\n",
    "На практике это означает заполнение двумерных массивов размерности: (длина последовательности) $\\times$ (количество возможных состояний). Когда массивы заполнены, $\\arg \\max_{k} \\delta(k, N)$ говорит о последнем состоянии. Начиная с него можно восстановить все состояния по массиву $s$. \n",
    "\n",
    "Осталось уточнить, как стартовать последовательный пересчет (чем заполнить первый столбец массива вероятностей):\n",
    "\n",
    "$$\\delta(k, 1) = p(k) p(x_1|t_1=k)$$\n",
    "\n",
    "В реализации рекомендуется перейти к логарифмам, т.к. произведение большого числа маленьких вероятностей может приводить к вычислительным ошибкам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def viterbi_algorithm(test_tokens_list, p_t, p_w_t, p_t_t):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        test_tokens_list: list of tokens. \n",
    "            Example: \n",
    "            ['I', 'go']\n",
    "        p_t: dict(float), tag->proba\n",
    "        p_w_t: - dict(dict(float), tag -> word -> proba\n",
    "        p_t_t: - dict(dict(float), previous_tag -> tag -> proba\n",
    "\n",
    "    Returns:\n",
    "        list of hidden tags\n",
    "    \"\"\"\n",
    "    words = 56057\n",
    "    INF = 10000000\n",
    "    # number of states\n",
    "    s = len(p_t)\n",
    "    # number of tokens\n",
    "    n = len(test_tokens_list)\n",
    "    \n",
    "    # initialization\n",
    "    viterbi  = dict()\n",
    "    backpointers = dict()\n",
    "    \n",
    "    # initial prob\n",
    "    for tag in p_t.keys():\n",
    "        viterbi[(0, tag)] = math.log(p_t[tag]) + math.log(p_w_t[tag].get(test_tokens_list[0], 1/words))\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        for tag in p_t.keys():\n",
    "            viterbi[(i, tag)] = -INF\n",
    "            for prev_tag in p_t.keys():\n",
    "                if viterbi[(i - 1, prev_tag)] + math.log(p_t_t[prev_tag].get(tag, 1/words)) + math.log(p_w_t[tag].get(test_tokens_list[i], 1/words)) >= viterbi[(i, tag)]:\n",
    "                    viterbi[(i, tag)] = viterbi[(i - 1, prev_tag)] + math.log(p_t_t[prev_tag].get(tag, 1/words)) + math.log(p_w_t[tag].get(test_tokens_list[i], 1/words))\n",
    "                    backpointers[(i, tag)] = prev_tag\n",
    "    \n",
    "    final_tag = ''\n",
    "    final_prob = -INF\n",
    "    for tag in p_t.keys():\n",
    "        if viterbi[(n - 1, tag)] >= final_prob:\n",
    "            final_prob = viterbi[(n - 1, tag)]\n",
    "            final_tag = tag\n",
    "    \n",
    "    result = [final_tag]\n",
    "    tag = final_tag\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        result.append(backpointers[(i, tag)])\n",
    "        tag = backpointers[(i, tag)]\n",
    "        \n",
    "    return list(reversed(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте работу реализованного алгоритма на следующих модельных примерах, проинтерпретируйте результат.\n",
    "\n",
    "- 'he can stay'\n",
    "- 'a milk can'\n",
    "- 'i saw a dog'\n",
    "- 'an old saw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_t, p_w_t, p_t_t = train_hmm(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'VERB', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "print(list(viterbi_algorithm(['he', 'can', 'stay'], p_t, p_w_t, p_t_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'NOUN', 'VERB', 'VERB', 'ADP', 'PRT']\n"
     ]
    }
   ],
   "source": [
    "print(list(viterbi_algorithm(['a', 'milk', 'can', 'fly', 'after', 'all'], p_t, p_w_t, p_t_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примените модель к отложенной выборке Брауновского корпуса и подсчитайте точность определения тегов (accuracy). Сделайте выводы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
